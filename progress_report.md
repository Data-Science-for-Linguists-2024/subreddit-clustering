# Progress Reports

## February 10 2024
- Created github repository
- Wrote project plan

## 1st Progress report
In this first leg of the process, I focused primarily on data collection. I wanted a big list of subreddits, but don't use reddit frequently enough to know more than a few off the top of my head, so instead I decided to scrape reddit's [list of popular subreddits](https://www.reddit.com/best/communities/1/). This has a few problems, such as bizarrely not including popular subreddits such as r/AmItheAsshole, but I couldn't find any other extensive lists like this outside of reddit, and it's good enough for my purposes. To get the list from this, I scraped the first 10 pages with [Beautiful Soup](https://pypi.org/project/beautifulsoup4/) gathering a total list of 2500 subreddits. This is probably way more than I actually need, but I wanted to get a big list at the start in case I decide to scale up my project later. For the code I used to do this, see [scripts/scrape_top_subreddits.py](https://github.com/Data-Science-for-Linguists-2024/subreddit-clustering/blob/main/scripts/scrape_top_subreddits.py). Next, I used this list and [PRAW](https://pypi.org/project/praw/) to scrape the comments from the top 10 posts of the past year for each subreddit. It took some trial and error to figure out what I was doing, and it was extremely slow so I had to leave my computer on overnight twice (scraping the whole list took around 9 hours and I forgot to save the ids the first time and had to do it twice...), but I did eventually manage to get the data I needed. for the code I used to do this, see [scripts/scrape_comments.py](https://github.com/Data-Science-for-Linguists-2024/subreddit-clustering/blob/main/scripts/scrape_comments.py). I also split the data up into several versions of increasing size (10, 100, 1000, and 2500 subreddits), so that I can experiment with smaller datasets and scale up if I want. Finally, I did some basic exploration of the dataset in [notebooks/data_exploration.ipynb](https://github.com/Data-Science-for-Linguists-2024/subreddit-clustering/blob/main/notebooks/data_exploration.ipynb).

For sharing my data, I want to do something similar to the GUM corpus, i'll distribute the data with the `text` column blanked out, and provide a script to re-fetch the data from reddit using the `comment_id` column, allowing anyone to reconstruct my exact dataset. I also provided a very small sample of the data in [data_samples/comment_data_sample.json](https://github.com/Data-Science-for-Linguists-2024/subreddit-clustering/blob/main/data_samples/comment_data_sample.json).
